{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import transformers\n",
    "import ast\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from tokenizers.normalizers import BertNormalizer\n",
    "import os\n",
    "import re\n",
    "from bert_score import score as bert_score\n",
    "from rouge_score import rouge_scorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_gpu_utilization():\n",
    "    \"\"\"Print GPU memory usage statistics for all available GPUs\"\"\"\n",
    "    print(\"\\nGPU Memory Usage:\")\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(f\"GPU {i}: {torch.cuda.memory_allocated(i) / 1024**3:.2f} GB / {torch.cuda.get_device_properties(i).total_memory / 1024**3:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embeddings(embeddings_csv_path, text_column=\"document\", embedding_column=\"embedding\"):\n",
    "    \"\"\"Load embeddings from CSV file\"\"\"\n",
    "    print(f\"Loading embeddings from {embeddings_csv_path}...\")\n",
    "    df = pd.read_csv(embeddings_csv_path)\n",
    "    \n",
    "    # Check if the necessary columns exist\n",
    "    if text_column not in df.columns:\n",
    "        raise ValueError(f\"Text column '{text_column}' not found in CSV. Columns: {df.columns.tolist()}\")\n",
    "    \n",
    "    if embedding_column not in df.columns:\n",
    "        raise ValueError(f\"Embedding column '{embedding_column}' not found in CSV. Columns: {df.columns.tolist()}\")\n",
    "    \n",
    "    # Parse embeddings from string to numpy arrays\n",
    "    print(\"Parsing embeddings from string format...\")\n",
    "    embeddings = []\n",
    "    \n",
    "    # Sample the first embedding to check format\n",
    "    sample_embedding = df[embedding_column].iloc[0]\n",
    "    \n",
    "    try:\n",
    "        # Try parsing as a list literal\n",
    "        parsed_embedding = ast.literal_eval(sample_embedding)\n",
    "        print(f\"Detected embedding format: Python list with {len(parsed_embedding)} dimensions\")\n",
    "        \n",
    "        # Parse all embeddings\n",
    "        for emb_str in df[embedding_column]:\n",
    "            embeddings.append(np.array(ast.literal_eval(emb_str)))\n",
    "        \n",
    "    except (ValueError, SyntaxError):\n",
    "        # If not a list literal, try parsing as comma-separated values\n",
    "        print(\"Trying comma-separated format...\")\n",
    "        try:\n",
    "            parsed_embedding = np.array([float(x) for x in sample_embedding.strip('[]').split(',')])\n",
    "            print(f\"Detected embedding format: CSV with {len(parsed_embedding)} dimensions\")\n",
    "            \n",
    "            # Parse all embeddings\n",
    "            for emb_str in df[embedding_column]:\n",
    "                values = [float(x) for x in emb_str.strip('[]').split(',')]\n",
    "                embeddings.append(np.array(values))\n",
    "                \n",
    "        except Exception as e:\n",
    "            raise ValueError(f\"Failed to parse embeddings: {e}. Please check the format.\")\n",
    "    \n",
    "    # Convert to numpy array for faster processing\n",
    "    embeddings = np.vstack(embeddings)\n",
    "    print(f\"Successfully loaded {len(embeddings)} embeddings with {embeddings.shape[1]} dimensions\")\n",
    "    \n",
    "    return df, embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_embeddings(embeddings, df, text_column):\n",
    "    \"\"\"Normalize document embeddings for better cosine similarity\"\"\"\n",
    "    print(\"Normalizing document embeddings...\")\n",
    "    # Remove any zero vectors\n",
    "    zero_indices = np.where(np.linalg.norm(embeddings, axis=1) == 0)[0]\n",
    "    if len(zero_indices) > 0:\n",
    "        print(f\"Warning: Found {len(zero_indices)} zero vectors in embeddings. Replacing with small random values.\")\n",
    "        for idx in zero_indices:\n",
    "            embeddings[idx] = np.random.normal(0, 0.01, embeddings.shape[1])\n",
    "    \n",
    "    # L2 normalize embeddings for more accurate cosine similarity\n",
    "    embeddings = embeddings / np.linalg.norm(embeddings, axis=1, keepdims=True)\n",
    "    \n",
    "    # Calculate and store document statistics for weighting\n",
    "    doc_lengths = np.array([len(str(doc).split()) for doc in df[text_column]])\n",
    "    avg_doc_length = np.mean(doc_lengths)\n",
    "    \n",
    "    print(f\"Average document length: {avg_doc_length:.2f} words\")\n",
    "    \n",
    "    return embeddings, doc_lengths, avg_doc_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_models(use_bert_base=True, llm_model_id=\"/kaggle/input/llama-3.1/transformers/8b/2\"):\n",
    "    \"\"\"Initialize embedding and LLM models\"\"\"\n",
    "    gpu_count = torch.cuda.device_count()\n",
    "    print(f\"Found {gpu_count} GPUs\")\n",
    "    \n",
    "    if gpu_count < 2:\n",
    "        print(\"Warning: Less than 2 GPUs detected. Will use available resources.\")\n",
    "    \n",
    "    # Clear GPU caches before loading models\n",
    "    for i in range(gpu_count):\n",
    "        with torch.cuda.device(i):\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    print_gpu_utilization()\n",
    "    \n",
    "    # Load embedding model for queries on GPU 0\n",
    "    if use_bert_base:\n",
    "        print(\"Loading BERT Base Uncased for query embeddings on GPU 0...\")\n",
    "        # Move to GPU 0\n",
    "        embedding_device = \"cuda:0\"\n",
    "        \n",
    "        # Initialize BERT Base Uncased with improved settings\n",
    "        tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "        embedding_model = AutoModel.from_pretrained('bert-base-uncased', trust_remote_code=True).to(embedding_device)\n",
    "        \n",
    "        # Load normalizer with simple settings\n",
    "        norm = BertNormalizer(lowercase=True, strip_accents=True, clean_text=True)\n",
    "    \n",
    "    print_gpu_utilization()\n",
    "    \n",
    "    # Load LLM on GPU 1 (or distributed across GPUs if needed)\n",
    "    print(f\"Loading LLM from {llm_model_id}...\")\n",
    "    \n",
    "    # Configure device map for model distribution\n",
    "    if gpu_count >= 2:\n",
    "        device_map = \"balanced\"  # Distribute across multiple GPUs\n",
    "    else:\n",
    "        device_map = \"auto\"      # Let Transformers decide\n",
    "\n",
    "    # Set memory optimization parameters\n",
    "    os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "    \n",
    "    # Load LLM with proper configuration\n",
    "    pipeline = transformers.pipeline(\n",
    "        \"text-generation\", \n",
    "        model=llm_model_id, \n",
    "        model_kwargs={\n",
    "            \"torch_dtype\": torch.bfloat16,\n",
    "            \"device_map\": device_map,\n",
    "            \"offload_folder\": \"/tmp/offload\"  # Enable offloading if needed\n",
    "        }\n",
    "    )\n",
    "\n",
    "    print_gpu_utilization()        \n",
    "    print(\"Model initialization complete\")\n",
    "    \n",
    "    return tokenizer, embedding_model, norm, pipeline, gpu_count, embedding_device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_text(text, norm):\n",
    "    \"\"\"Normalize text using simple BERT normalizer\"\"\"\n",
    "    # Handle empty or None input\n",
    "    if not text:\n",
    "        return \"\"\n",
    "        \n",
    "    # Apply basic preprocessing\n",
    "    text = text.strip()\n",
    "    \n",
    "    # Split into lines and normalize each line\n",
    "    text = [norm.normalize_str(s) for s in text.split('\\n')]\n",
    "    \n",
    "    # Remove redundant spaces and join\n",
    "    normalized_text = ' '.join([re.sub(r'\\s+', ' ', s).strip() for s in text])\n",
    "    \n",
    "    return normalized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_query(query):\n",
    "    \"\"\"Expand the query with related terms to improve matching\"\"\"\n",
    "    # Simple rule-based expansion\n",
    "    # This could be replaced with a more sophisticated approach\n",
    "    \n",
    "    # Extract key terms (simple approach)\n",
    "    terms = re.findall(r'\\b\\w+\\b', query.lower())\n",
    "    \n",
    "    # Filter out common stop words (simplified list)\n",
    "    stop_words = {'the', 'a', 'an', 'and', 'or', 'but', 'is', 'are', 'was', 'were', \n",
    "                 'in', 'on', 'at', 'to', 'for', 'with', 'by', 'about', 'like', 'from'}\n",
    "    \n",
    "    key_terms = [term for term in terms if term not in stop_words and len(term) > 2]\n",
    "    \n",
    "    # If no key terms found, return original query\n",
    "    if not key_terms:\n",
    "        return query\n",
    "        \n",
    "    # Add original query plus key terms for emphasis\n",
    "    expanded_query = query + \" \" + \" \".join(key_terms)\n",
    "    \n",
    "    return expanded_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_query_embedding(query, tokenizer, embedding_model, embedding_device, norm, use_bert_base=True, embedding_pooling='mean_cls'):\n",
    "    \"\"\"Generate better embedding for a query text using optimized pooling strategies\"\"\"\n",
    "    if not use_bert_base:\n",
    "        raise ValueError(\"Only BERT Base Uncased embedding is currently supported\")\n",
    "    \n",
    "    # Apply query expansion to improve matching\n",
    "    expanded_query = expand_query(query)\n",
    "    \n",
    "    # Normalize query\n",
    "    normalized_query = normalize_text(expanded_query, norm)\n",
    "    \n",
    "    # Move computation to GPU 0\n",
    "    with torch.cuda.device(0):\n",
    "        # Tokenize with increased max length for better coverage\n",
    "        tokenized_query = tokenizer(normalized_query, padding=True, \n",
    "                                   truncation=True, max_length=512, return_tensors='pt').to(embedding_device)\n",
    "        \n",
    "        # Get embeddings with attention mask\n",
    "        with torch.no_grad():\n",
    "            outputs = embedding_model(**tokenized_query)\n",
    "            \n",
    "            # Get last hidden state and attention mask\n",
    "            last_hidden_state = outputs.last_hidden_state\n",
    "            attention_mask = tokenized_query['attention_mask']\n",
    "            \n",
    "            # Choose pooling strategy based on configuration\n",
    "            if embedding_pooling == 'cls':\n",
    "                # CLS token pooling (first token)\n",
    "                query_embedding = last_hidden_state[:, 0].cpu().numpy()[0]\n",
    "                \n",
    "            elif embedding_pooling == 'mean':\n",
    "                # Mean pooling with attention mask\n",
    "                input_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n",
    "                sum_embeddings = torch.sum(last_hidden_state * input_mask_expanded, 1)\n",
    "                sum_mask = input_mask_expanded.sum(1)\n",
    "                sum_mask = torch.clamp(sum_mask, min=1e-9)\n",
    "                query_embedding = (sum_embeddings / sum_mask).cpu().numpy()[0]\n",
    "                \n",
    "            else:  # 'mean_cls' (default)\n",
    "                # Weighted combination of CLS and mean pooling\n",
    "                # CLS token embedding\n",
    "                cls_embedding = last_hidden_state[:, 0]\n",
    "                \n",
    "                # Mean pooling with attention mask\n",
    "                input_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n",
    "                sum_embeddings = torch.sum(last_hidden_state * input_mask_expanded, 1)\n",
    "                sum_mask = input_mask_expanded.sum(1)\n",
    "                sum_mask = torch.clamp(sum_mask, min=1e-9)\n",
    "                mean_embedding = sum_embeddings / sum_mask\n",
    "                \n",
    "                # Combine both methods (0.7 for mean pooling, 0.3 for CLS token)\n",
    "                query_embedding = (0.7 * mean_embedding + 0.3 * cls_embedding).cpu().numpy()[0]\n",
    "    \n",
    "    # Normalize the embedding vector to unit length for proper cosine similarity\n",
    "    query_embedding = query_embedding / np.linalg.norm(query_embedding)\n",
    "    \n",
    "    return query_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_similar_texts(query_embedding, embeddings, df, text_column, doc_lengths, avg_doc_length, \n",
    "                      top_k=5, distance_metric='hybrid', similarity_threshold=0.6):\n",
    "    \"\"\"Find the most similar texts to the query embedding with improved similarity metrics\"\"\"\n",
    "    if distance_metric == 'cosine':\n",
    "        # Compute cosine similarity\n",
    "        similarities = cosine_similarity([query_embedding], embeddings)[0]\n",
    "        \n",
    "    elif distance_metric == 'dot':\n",
    "        # Compute dot product\n",
    "        similarities = np.dot(embeddings, query_embedding)\n",
    "        \n",
    "    elif distance_metric == 'hybrid':\n",
    "        # Hybrid approach: combine cosine similarity and BM25-inspired weighting\n",
    "        cosine_sim = cosine_similarity([query_embedding], embeddings)[0]\n",
    "        \n",
    "        # BM25-inspired length normalization (penalize very short and very long documents)\n",
    "        # k1 and b are BM25 parameters (can be tuned)\n",
    "        k1 = 1.5\n",
    "        b = 0.75\n",
    "        length_norm = (1 - b) + b * (doc_lengths / avg_doc_length)\n",
    "        bm25_weights = (k1 + 1) / (k1 * length_norm + 1)\n",
    "        \n",
    "        # Apply length normalization to cosine similarity\n",
    "        similarities = cosine_sim * bm25_weights\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(f\"Unknown distance metric: {distance_metric}\")\n",
    "    \n",
    "    # Apply similarity threshold to filter out less relevant documents\n",
    "    qualified_indices = np.where(similarities >= similarity_threshold)[0]\n",
    "    \n",
    "    # If no document passes the threshold, fall back to top k\n",
    "    if len(qualified_indices) == 0:\n",
    "        top_indices = similarities.argsort()[-top_k:][::-1]\n",
    "    else:\n",
    "        # Otherwise, get the top k from qualified documents\n",
    "        top_qualified_similarities = similarities[qualified_indices]\n",
    "        top_qualified_indices = top_qualified_similarities.argsort()[-min(top_k, len(qualified_indices)):][::-1]\n",
    "        top_indices = qualified_indices[top_qualified_indices]\n",
    "    \n",
    "    # Get results with reranking\n",
    "    results = []\n",
    "    for idx in top_indices:\n",
    "        doc_text = df.iloc[idx][text_column]\n",
    "        \n",
    "        # Calculate additional features for reranking\n",
    "        doc_length = len(str(doc_text).split())\n",
    "        length_penalty = 1.0  # No penalty by default\n",
    "        \n",
    "        # Apply a small penalty for very short or very long documents\n",
    "        if doc_length < 50:\n",
    "            length_penalty = 0.9  # Slight penalty for very short docs\n",
    "        elif doc_length > 1000:\n",
    "            length_penalty = 0.95  # Slight penalty for very long docs\n",
    "            \n",
    "        # Final score combining similarity and length consideration\n",
    "        final_score = similarities[idx] * length_penalty\n",
    "        \n",
    "        results.append({\n",
    "            \"index\": idx,\n",
    "            \"text\": doc_text,\n",
    "            \"similarity\": similarities[idx],\n",
    "            \"final_score\": final_score\n",
    "        })\n",
    "    \n",
    "    # Final sort by the combined score\n",
    "    results.sort(key=lambda x: x[\"final_score\"], reverse=True)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_context(similar_texts, question):\n",
    "    \"\"\"Enhanced context formatting to highlight key phrases\"\"\"\n",
    "    context = \"DIRECT CONTEXT FOR ANSWERING:\\n\\n\"\n",
    "    \n",
    "    # Extract key terms from the question\n",
    "    question_terms = set(re.findall(r'\\b\\w+\\b', question.lower()))\n",
    "    stop_words = {'the', 'a', 'an', 'and', 'or', 'but', 'is', 'are', 'was', 'were'}\n",
    "    key_terms = {term for term in question_terms if term not in stop_words and len(term) > 2}\n",
    "    \n",
    "    # Prioritize and weight reference texts\n",
    "    scored_texts = []\n",
    "    for result in similar_texts:\n",
    "        doc_text = result['text']\n",
    "        \n",
    "        # Score based on key term matches\n",
    "        term_matches = sum(1 for term in key_terms if term in doc_text.lower())\n",
    "        \n",
    "        # Additional scoring based on similarity and length\n",
    "        score = result['similarity'] * (1 + term_matches / len(key_terms))\n",
    "        scored_texts.append((doc_text, score))\n",
    "    \n",
    "    # Sort texts by weighted score\n",
    "    scored_texts.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # Truncate to top most relevant texts\n",
    "    top_texts = scored_texts[:3]\n",
    "    \n",
    "    # Format context with clear markers\n",
    "    for i, (text, score) in enumerate(top_texts, 1):\n",
    "        context += f\"REFERENCE {i} (Relevance Score: {score:.2f}):\\n{text}\\n\\n\"\n",
    "    \n",
    "    return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answer(question, context, pipeline):\n",
    "    \"\"\"Generate answer using the LLM with improved prompt engineering\"\"\"\n",
    "    system_instruction = \"\"\"You are a precise and informative assistant. When answering questions:\n",
    "1. Directly use phrases and key information from the provided context.\n",
    "2. Aim to closely match the language and terminology of the source documents.\n",
    "3. Cite specific details from the context whenever possible.\n",
    "4. If you cannot find a definitive answer, clearly state that.\n",
    "5. Prioritize reproducing key information over generating entirely novel text.\"\"\"\n",
    "\n",
    "    prompt = f\"\"\"{system_instruction}\n",
    "\n",
    "CONTEXT:\n",
    "{context}\n",
    "\n",
    "IMPORTANT INSTRUCTIONS:\n",
    "- Use the exact wording from the context when relevant\n",
    "- Directly incorporate phrases from the provided documents\n",
    "- Ensure your answer closely reflects the source information\n",
    "\n",
    "QUESTION: {question}\n",
    "\n",
    "ANSWER:\"\"\"\n",
    "\n",
    "    # Generate answer with context-aware parameters\n",
    "    try:\n",
    "        response = pipeline(\n",
    "            prompt,\n",
    "            max_new_tokens=512,\n",
    "            do_sample=True,\n",
    "            temperature=0.3,  # Lower temperature for more deterministic output\n",
    "            top_p=0.8,        # Tighter probability mass\n",
    "            top_k=30,         # More focused token selection\n",
    "            repetition_penalty=1.2,  # Encourage using context phrases\n",
    "            batch_size=1\n",
    "        )[0]['generated_text']\n",
    "        \n",
    "        # Extract just the answer part\n",
    "        answer = response[len(prompt):].strip()\n",
    "        \n",
    "        # Fallback if answer is empty\n",
    "        if not answer:\n",
    "            answer = \"I apologize, but I couldn't generate a substantive answer based on the available context.\"\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error generating answer: {e}\")\n",
    "        answer = \"I apologize, but I encountered an error while trying to generate an answer.\"\n",
    "    \n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_answer(answer, similar_texts):\n",
    "    \"\"\"Evaluate the generated answer using BERTScore and ROUGE metrics\"\"\"\n",
    "    evaluation = {\n",
    "        \"bert_scores\": {},\n",
    "        \"rouge_scores\": {}\n",
    "    }\n",
    "    \n",
    "    # Calculate BERTScore\n",
    "    try:\n",
    "        # Prepare reference texts\n",
    "        reference_texts = [doc['text'] for doc in similar_texts[:3]]\n",
    "        \n",
    "        # Ensure texts are not empty and have sufficient length\n",
    "        reference_texts = [ref for ref in reference_texts if ref and len(ref.strip()) > 10]\n",
    "        \n",
    "        if reference_texts and answer and len(answer.strip()) > 0:\n",
    "            # If only one reference text, repeat it to match the number of candidates\n",
    "            if len(reference_texts) == 1:\n",
    "                reference_texts = reference_texts * 3\n",
    "            \n",
    "            # Truncate references to match candidates if needed\n",
    "            reference_texts = reference_texts[:3]\n",
    "            \n",
    "            # Ensure we have multiple references\n",
    "            if len(reference_texts) > 1:\n",
    "                P, R, F1 = bert_score(\n",
    "                    cands=[answer] * len(reference_texts),  # Repeat answer to match references\n",
    "                    refs=reference_texts, \n",
    "                    lang=\"en\", \n",
    "                    verbose=False,\n",
    "                    model_type='bert-base-uncased'\n",
    "                )\n",
    "                \n",
    "                bert_scores = {\n",
    "                    'precision': float(P.mean()),\n",
    "                    'recall': float(R.mean()),\n",
    "                    'f1': float(F1.mean())\n",
    "                }\n",
    "                evaluation[\"bert_scores\"] = bert_scores\n",
    "    except Exception as e:\n",
    "        print(f\"BERTScore calculation error: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "    \n",
    "    # Calculate ROUGE scores\n",
    "    try:\n",
    "        scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "        \n",
    "        # Prepare references with different preprocessing\n",
    "        reference_texts = []\n",
    "        for doc in similar_texts[:3]:\n",
    "            # Original text\n",
    "            reference_texts.append(doc['text'])\n",
    "            \n",
    "            # Extractive summary (first and last sentences)\n",
    "            sentences = re.split(r'(?<=[.!?])\\s+', doc['text'])\n",
    "            extractive_ref = sentences[0] + \" \" + sentences[-1] if len(sentences) > 1 else doc['text']\n",
    "            reference_texts.append(extractive_ref)\n",
    "        \n",
    "        # Collect ROUGE scores across multiple references\n",
    "        all_rouge_results = []\n",
    "        for ref_text in reference_texts:\n",
    "            if ref_text and len(ref_text.strip()) > 10 and answer and len(answer.strip()) > 0:\n",
    "                rouge_result = scorer.score(ref_text, answer)\n",
    "                all_rouge_results.append(rouge_result)\n",
    "        \n",
    "        # Aggregate ROUGE scores\n",
    "        if all_rouge_results:\n",
    "            rouge_metrics = ['rouge1', 'rouge2', 'rougeL']\n",
    "            rouge_scores = {metric: {\n",
    "                'precision': np.mean([result[metric].precision for result in all_rouge_results]),\n",
    "                'recall': np.mean([result[metric].recall for result in all_rouge_results]),\n",
    "                'fmeasure': np.mean([result[metric].fmeasure for result in all_rouge_results])\n",
    "            } for metric in rouge_metrics}\n",
    "            \n",
    "            evaluation[\"rouge_scores\"] = rouge_scores\n",
    "    except Exception as e:\n",
    "        print(f\"ROUGE score calculation error: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "    \n",
    "    return evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_question(question, df, embeddings, tokenizer, embedding_model, norm, pipeline, \n",
    "                   gpu_count, embedding_device, doc_lengths, avg_doc_length, \n",
    "                   text_column=\"document\", top_k=5, use_bert_base=True, \n",
    "                   distance_metric='hybrid', embedding_pooling='mean_cls', \n",
    "                   similarity_threshold=0.6):\n",
    "    \"\"\"\n",
    "    Answer a question using RAG framework with improved context handling and response evaluation\n",
    "    \n",
    "    Args:\n",
    "        question: The user's question\n",
    "        df: DataFrame containing documents\n",
    "        embeddings: Document embeddings\n",
    "        tokenizer: BERT tokenizer\n",
    "        embedding_model: BERT model for embeddings\n",
    "        norm: Text normalizer\n",
    "        pipeline: LLM pipeline\n",
    "        gpu_count: Number of available GPUs\n",
    "        embedding_device: Device for embedding model\n",
    "        doc_lengths: Array of document lengths\n",
    "        avg_doc_length: Average document length\n",
    "        text_column: Name of column containing text\n",
    "        top_k: Number of top results to retrieve\n",
    "        use_bert_base: Whether to use BERT Base Uncased\n",
    "        distance_metric: Similarity metric to use\n",
    "        embedding_pooling: Pooling strategy\n",
    "        similarity_threshold: Minimum similarity score\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with answer, top results, and evaluation metrics\n",
    "    \"\"\"\n",
    "    print(\"\\nProcessing query...\")\n",
    "    \n",
    "    # Clear GPU 0 cache\n",
    "    with torch.cuda.device(0):\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.synchronize()\n",
    "    \n",
    "    print(\"Getting query embedding on GPU 0...\")\n",
    "    # Get query embedding\n",
    "    query_embedding = get_query_embedding(\n",
    "        question, tokenizer, embedding_model, embedding_device, norm, \n",
    "        use_bert_base, embedding_pooling\n",
    "    )\n",
    "    \n",
    "    print(\"Finding similar texts...\")\n",
    "    # Find similar texts\n",
    "    similar_texts = find_similar_texts(\n",
    "        query_embedding, embeddings, df, text_column, doc_lengths, avg_doc_length,\n",
    "        top_k, distance_metric, similarity_threshold\n",
    "    )\n",
    "    \n",
    "    # Check if we have any similar texts\n",
    "    if not similar_texts:\n",
    "        return {\n",
    "            \"answer\": \"I couldn't find any relevant information to answer your question.\",\n",
    "            \"top_results\": [],\n",
    "            \"evaluation\": {\n",
    "                \"bert_scores\": {},\n",
    "                \"rouge_scores\": {}\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    # Format context for LLM with improved chunking\n",
    "    context = format_context(similar_texts, question)\n",
    "    \n",
    "    # Clear GPU caches before LLM inference\n",
    "    for i in range(gpu_count):\n",
    "        with torch.cuda.device(i):\n",
    "            torch.cuda.empty_cache()\n",
    "            torch.cuda.synchronize()\n",
    "    \n",
    "    print_gpu_utilization()\n",
    "    print(\"Generating answer with LLM...\")\n",
    "    \n",
    "    # Generate answer\n",
    "    answer = generate_answer(question, context, pipeline)\n",
    "    \n",
    "    # Evaluate answer\n",
    "    evaluation = evaluate_answer(answer, similar_texts)\n",
    "    \n",
    "    print(\"Processing complete.\")\n",
    "    print_gpu_utilization()\n",
    "    \n",
    "    return {\n",
    "        \"answer\": answer,\n",
    "        \"top_results\": similar_texts,\n",
    "        \"evaluation\": evaluation\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demo_rag_system():\n",
    "    \"\"\"Run a demonstration of the RAG system\"\"\"\n",
    "    # Check if the model exists\n",
    "    model_id = \"/kaggle/input/llama-3.1/transformers/8b/2\"\n",
    "    \n",
    "    # Initialize the RAG system with optimized parameters\n",
    "    print(\"Loading embeddings...\")\n",
    "    df, embeddings = load_embeddings(\n",
    "        embeddings_csv_path=\"/kaggle/input/bert-embeddings/bert_embeddings.csv\",\n",
    "        text_column=\"text_preview\",\n",
    "        embedding_column=\"embeddings\"\n",
    "    )\n",
    "    \n",
    "    print(\"Preprocessing embeddings...\")\n",
    "    embeddings, doc_lengths, avg_doc_length = preprocess_embeddings(embeddings, df, \"text_preview\")\n",
    "    \n",
    "    print(\"Initializing models...\")\n",
    "    tokenizer, embedding_model, norm, pipeline, gpu_count, embedding_device = initialize_models(\n",
    "        use_bert_base=True,\n",
    "        llm_model_id=model_id\n",
    "    )\n",
    "    \n",
    "    # Interactive query loop\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"Optimized RAG Question Answering System\")\n",
    "    print(\"=\"*50)\n",
    "    print(\"Type 'exit' to quit\\n\")\n",
    "    \n",
    "    while True:\n",
    "        question = input(\"\\nEnter your question: \")\n",
    "        if question.lower() == 'exit':\n",
    "            break\n",
    "        \n",
    "        print(\"\\nProcessing...\")\n",
    "        try:\n",
    "            result = answer_question(\n",
    "                question, df, embeddings, tokenizer, embedding_model, norm, pipeline,\n",
    "                gpu_count, embedding_device, doc_lengths, avg_doc_length,\n",
    "                text_column=\"text_preview\", top_k=5, use_bert_base=True,\n",
    "                distance_metric='hybrid', embedding_pooling='mean_cls',\n",
    "                similarity_threshold=0.55\n",
    "            )\n",
    "            \n",
    "            print(\"\\n\" + \"=\"*50)\n",
    "            print(\"ANSWER:\")\n",
    "            print(\"=\"*50)\n",
    "            print(result[\"answer\"])\n",
    "            \n",
    "            print(\"\\n\" + \"=\"*50)\n",
    "            print(\"ANSWER EVALUATION:\")\n",
    "            print(\"=\"*50)\n",
    "            \n",
    "            # BERTScore handling\n",
    "            bert_scores = result[\"evaluation\"].get(\"bert_scores\", {})\n",
    "            if bert_scores:\n",
    "                print(\"BERTScore:\")\n",
    "                for metric, value in bert_scores.items():\n",
    "                    print(f\"  {metric.capitalize()}: {value:.4f}\")\n",
    "            else:\n",
    "                print(\"BERTScore: No scores available\")\n",
    "            \n",
    "            # ROUGE Scores handling\n",
    "            rouge_scores = result[\"evaluation\"].get(\"rouge_scores\", {})\n",
    "            if rouge_scores:\n",
    "                print(\"\\nROUGE Scores:\")\n",
    "                for metric, scores in rouge_scores.items():\n",
    "                    print(f\"  {metric.upper()}:\")\n",
    "                    for score_type, value in scores.items():\n",
    "                        print(f\"    {score_type.capitalize()}: {value:.4f}\")\n",
    "            else:\n",
    "                print(\"ROUGE Scores: No scores available\")\n",
    "            \n",
    "            print(\"\\n\" + \"=\"*50)\n",
    "            print(\"TOP RELEVANT DOCUMENTS:\")\n",
    "            print(\"=\"*50)\n",
    "            for i, doc in enumerate(result[\"top_results\"], 1):\n",
    "                print(f\"{i}. Similarity: {doc['similarity']:.4f}, Final Score: {doc['final_score']:.4f}\")\n",
    "                # Limit text display to avoid overwhelming console\n",
    "                text_preview = doc[\"text\"]\n",
    "                if len(text_preview) > 300:\n",
    "                    text_preview = text_preview[:300] + \"...\"\n",
    "                print(f\"{text_preview}\\n\")\n",
    "        except Exception as e:\n",
    "            print(f\"\\nERROR: {str(e)}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            \n",
    "        except RuntimeError as e:\n",
    "            if \"CUDA out of memory\" in str(e):\n",
    "                print(\"\\nERROR: CUDA out of memory. Try clearing GPU cache and reducing model parameters.\")\n",
    "                \n",
    "                # Clear GPU caches\n",
    "                for i in range(torch.cuda.device_count()):\n",
    "                    with torch.cuda.device(i):\n",
    "                        torch.cuda.empty_cache()\n",
    "                        torch.cuda.synchronize()\n",
    "                \n",
    "                print_gpu_utilization()\n",
    "            else:\n",
    "                print(f\"\\nERROR: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Set memory optimization environment variable\n",
    "    os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "    \n",
    "    # Start demo\n",
    "    demo_rag_system()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
