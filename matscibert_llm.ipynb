{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import transformers\n",
    "import ast\n",
    "import re\n",
    "import os\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from tokenizers.normalizers import BertNormalizer\n",
    "from bert_score import score as bert_score\n",
    "from rouge_score import rouge_scorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_gpu_utilization():\n",
    "    \"\"\"Print GPU memory usage statistics for all available GPUs\"\"\"\n",
    "    print(\"\\nGPU Memory Usage:\")\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(f\"GPU {i}: {torch.cuda.memory_allocated(i) / 1024**3:.2f} GB / {torch.cuda.get_device_properties(i).total_memory / 1024**3:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_vocab_mappings(file_path):\n",
    "    \"\"\"Load vocabulary mappings from file\"\"\"\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        mappings = f.read().strip().split('\\n')\n",
    "    \n",
    "    return {m[0]: m[2:] for m in mappings}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_text(text, normalizer=None, mappings=None):\n",
    "    \"\"\"Normalize text using MatSciBERT normalizer with improved handling\"\"\"\n",
    "    # Handle empty or None input\n",
    "    if not text:\n",
    "        return \"\"\n",
    "        \n",
    "    # Apply basic preprocessing\n",
    "    text = text.strip()\n",
    "    \n",
    "    # Split into lines and normalize each line\n",
    "    text = [normalizer.normalize_str(s) if normalizer else s for s in text.split('\\n')]\n",
    "    out = []\n",
    "    for s in text:\n",
    "        if mappings:\n",
    "            # Apply mappings with better handling of special characters\n",
    "            norm_s = ''.join(mappings.get(c, c if c.isalnum() else ' ') for c in s)\n",
    "            # Remove redundant spaces\n",
    "            norm_s = re.sub(r'\\s+', ' ', norm_s).strip()\n",
    "        else:\n",
    "            norm_s = s\n",
    "        out.append(norm_s)\n",
    "    \n",
    "    return '\\n'.join(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_query(query):\n",
    "    \"\"\"Expand the query with related terms to improve matching\"\"\"\n",
    "    # Simple rule-based expansion\n",
    "    # Extract key terms (simple approach)\n",
    "    terms = re.findall(r'\\b\\w+\\b', query.lower())\n",
    "    \n",
    "    # Filter out common stop words (simplified list)\n",
    "    stop_words = {'the', 'a', 'an', 'and', 'or', 'but', 'is', 'are', 'was', 'were', \n",
    "                 'in', 'on', 'at', 'to', 'for', 'with', 'by', 'about', 'like', 'from'}\n",
    "    \n",
    "    key_terms = [term for term in terms if term not in stop_words and len(term) > 2]\n",
    "    \n",
    "    # If no key terms found, return original query\n",
    "    if not key_terms:\n",
    "        return query\n",
    "        \n",
    "    # Add original query plus key terms for emphasis\n",
    "    expanded_query = query + \" \" + \" \".join(key_terms)\n",
    "    \n",
    "    return expanded_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_embeddings(embeddings):\n",
    "    \"\"\"Normalize document embeddings for better cosine similarity\"\"\"\n",
    "    print(\"Normalizing document embeddings...\")\n",
    "    # Remove any zero vectors\n",
    "    zero_indices = np.where(np.linalg.norm(embeddings, axis=1) == 0)[0]\n",
    "    if len(zero_indices) > 0:\n",
    "        print(f\"Warning: Found {len(zero_indices)} zero vectors in embeddings. Replacing with small random values.\")\n",
    "        for idx in zero_indices:\n",
    "            embeddings[idx] = np.random.normal(0, 0.01, embeddings.shape[1])\n",
    "    \n",
    "    # L2 normalize embeddings for more accurate cosine similarity\n",
    "    normalized_embeddings = embeddings / np.linalg.norm(embeddings, axis=1, keepdims=True)\n",
    "    \n",
    "    return normalized_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_query_embedding(query, embedding_model, tokenizer, device, pooling='mean_cls'):\n",
    "    \"\"\"Generate embedding for a query text using optimized pooling strategies\"\"\"\n",
    "    # Apply query expansion to improve matching\n",
    "    expanded_query = expand_query(query)\n",
    "    \n",
    "    # Normalize query (simplified, you might want to pass a normalizer if needed)\n",
    "    normalized_query = normalize_text(expanded_query)\n",
    "    \n",
    "    # Move computation to specified device\n",
    "    with torch.cuda.device(device):\n",
    "        # Tokenize with increased max length for better coverage\n",
    "        tokenized_query = tokenizer(normalized_query, padding=True, \n",
    "                                   truncation=True, max_length=512, return_tensors='pt').to(device)\n",
    "        \n",
    "        # Get embeddings with attention mask\n",
    "        with torch.no_grad():\n",
    "            outputs = embedding_model(**tokenized_query)\n",
    "            \n",
    "            # Get last hidden state and attention mask\n",
    "            last_hidden_state = outputs.last_hidden_state\n",
    "            attention_mask = tokenized_query['attention_mask']\n",
    "            \n",
    "            # Choose pooling strategy\n",
    "            if pooling == 'cls':\n",
    "                # CLS token pooling (first token)\n",
    "                query_embedding = last_hidden_state[:, 0].cpu().numpy()[0]\n",
    "                \n",
    "            elif pooling == 'mean':\n",
    "                # Mean pooling with attention mask\n",
    "                input_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n",
    "                sum_embeddings = torch.sum(last_hidden_state * input_mask_expanded, 1)\n",
    "                sum_mask = input_mask_expanded.sum(1)\n",
    "                sum_mask = torch.clamp(sum_mask, min=1e-9)\n",
    "                query_embedding = (sum_embeddings / sum_mask).cpu().numpy()[0]\n",
    "                \n",
    "            else:  # 'mean_cls'\n",
    "                # Weighted combination of CLS and mean pooling\n",
    "                cls_embedding = last_hidden_state[:, 0]\n",
    "                \n",
    "                input_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n",
    "                sum_embeddings = torch.sum(last_hidden_state * input_mask_expanded, 1)\n",
    "                sum_mask = input_mask_expanded.sum(1)\n",
    "                sum_mask = torch.clamp(sum_mask, min=1e-9)\n",
    "                mean_embedding = sum_embeddings / sum_mask\n",
    "                \n",
    "                # Combine both methods (0.7 for mean pooling, 0.3 for CLS token)\n",
    "                query_embedding = (0.7 * mean_embedding + 0.3 * cls_embedding).cpu().numpy()[0]\n",
    "    \n",
    "    # Normalize the embedding vector to unit length for proper cosine similarity\n",
    "    query_embedding = query_embedding / np.linalg.norm(query_embedding)\n",
    "    \n",
    "    return query_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_similar_texts(query_embedding, embeddings, documents, \n",
    "                       top_k=5, \n",
    "                       distance_metric='hybrid', \n",
    "                       similarity_threshold=0.6):\n",
    "    \"\"\"Find the most similar texts to the query embedding\"\"\"\n",
    "    if distance_metric == 'cosine':\n",
    "        # Compute cosine similarity\n",
    "        similarities = cosine_similarity([query_embedding], embeddings)[0]\n",
    "        \n",
    "    elif distance_metric == 'dot':\n",
    "        # Compute dot product\n",
    "        similarities = np.dot(embeddings, query_embedding)\n",
    "        \n",
    "    elif distance_metric == 'hybrid':\n",
    "        # Hybrid approach: combine cosine similarity and BM25-inspired weighting\n",
    "        cosine_sim = cosine_similarity([query_embedding], embeddings)[0]\n",
    "        \n",
    "        # BM25-inspired length normalization\n",
    "        k1 = 1.5\n",
    "        b = 0.75\n",
    "        doc_lengths = np.array([len(str(doc).split()) for doc in documents])\n",
    "        avg_doc_length = np.mean(doc_lengths)\n",
    "        \n",
    "        length_norm = (1 - b) + b * (doc_lengths / avg_doc_length)\n",
    "        bm25_weights = (k1 + 1) / (k1 * length_norm + 1)\n",
    "        \n",
    "        # Apply length normalization to cosine similarity\n",
    "        similarities = cosine_sim * bm25_weights\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(f\"Unknown distance metric: {distance_metric}\")\n",
    "    \n",
    "    # Apply similarity threshold to filter out less relevant documents\n",
    "    qualified_indices = np.where(similarities >= similarity_threshold)[0]\n",
    "    \n",
    "    # If no document passes the threshold, fall back to top k\n",
    "    if len(qualified_indices) == 0:\n",
    "        top_indices = similarities.argsort()[-top_k:][::-1]\n",
    "    else:\n",
    "        # Otherwise, get the top k from qualified documents\n",
    "        top_qualified_similarities = similarities[qualified_indices]\n",
    "        top_qualified_indices = top_qualified_similarities.argsort()[-min(top_k, len(qualified_indices)):][::-1]\n",
    "        top_indices = qualified_indices[top_qualified_indices]\n",
    "    \n",
    "    # Get results with reranking\n",
    "    results = []\n",
    "    for idx in top_indices:\n",
    "        doc_text = documents[idx]\n",
    "        \n",
    "        # Calculate additional features for reranking\n",
    "        doc_length = len(str(doc_text).split())\n",
    "        length_penalty = 1.0  # No penalty by default\n",
    "        \n",
    "        # Apply a small penalty for very short or very long documents\n",
    "        if doc_length < 50:\n",
    "            length_penalty = 0.9  # Slight penalty for very short docs\n",
    "        elif doc_length > 1000:\n",
    "            length_penalty = 0.95  # Slight penalty for very long docs\n",
    "            \n",
    "        # Final score combining similarity and length consideration\n",
    "        final_score = similarities[idx] * length_penalty\n",
    "        \n",
    "        results.append({\n",
    "            \"index\": idx,\n",
    "            \"text\": doc_text,\n",
    "            \"similarity\": similarities[idx],\n",
    "            \"final_score\": final_score\n",
    "        })\n",
    "    \n",
    "    # Final sort by the combined score\n",
    "    results.sort(key=lambda x: x[\"final_score\"], reverse=True)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def highlight_relevant_parts(text, question):\n",
    "    # Extract key terms from the question\n",
    "    question_terms = set(re.findall(r'\\b\\w+\\b', question.lower()))\n",
    "    stop_words = {'the', 'a', 'an', 'and', 'or', 'but', 'is', 'are', 'was', 'were', \n",
    "                 'in', 'on', 'at', 'to', 'for', 'with', 'by', 'about', 'like', 'from'}\n",
    "    key_terms = {term for term in question_terms if term not in stop_words and len(term) > 2}\n",
    "    \n",
    "    if not key_terms or not text:\n",
    "        return text\n",
    "        \n",
    "    # Split text into sentences\n",
    "    sentences = re.split(r'(?<=[.!?])\\s+', text)\n",
    "    \n",
    "    # Score each sentence based on key term occurrence\n",
    "    scored_sentences = []\n",
    "    for sentence in sentences:\n",
    "        sentence_lower = sentence.lower()\n",
    "        score = sum(1 for term in key_terms if term in sentence_lower)\n",
    "        scored_sentences.append((sentence, score))\n",
    "    \n",
    "    # Sort sentences by score (descending)\n",
    "    scored_sentences.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # Take top sentences and restore original order\n",
    "    top_sentences = [s[0] for s in scored_sentences if s[1] > 0]\n",
    "    if not top_sentences:\n",
    "        # If no sentences contain key terms, return the first few sentences\n",
    "        top_sentences = sentences[:3]\n",
    "    \n",
    "    # Get original indices to preserve document flow\n",
    "    top_indices = [sentences.index(s) for s in top_sentences]\n",
    "    top_indices.sort()\n",
    "    \n",
    "    # Reconstruct text preserving original order\n",
    "    highlighted_text = \" \".join([sentences[i] for i in top_indices])\n",
    "    \n",
    "    # If highlighted text is too short, return more of the original\n",
    "    if len(highlighted_text) < len(text) * 0.3:\n",
    "        return text[:1000]  # Return first 1000 chars\n",
    "        \n",
    "    return highlighted_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_context(similar_texts, question, max_context_tokens=3000):\n",
    "    context = \"Here is the relevant information from the dataset:\\n\\n\"\n",
    "    \n",
    "    # Estimate tokens for each document and add until we hit the budget\n",
    "    current_tokens = 0\n",
    "    \n",
    "    for i, result in enumerate(similar_texts, 1):\n",
    "        doc_text = result['text']\n",
    "        similarity = result['similarity']\n",
    "        \n",
    "        # Highlight most relevant parts \n",
    "        highlighted_text = highlight_relevant_parts(doc_text, question)\n",
    "        \n",
    "        # Estimate token count (rough approximation: 4 chars ~= 1 token)\n",
    "        doc_token_estimate = len(highlighted_text) // 4\n",
    "        \n",
    "        # If adding this document would exceed the budget, truncate or skip\n",
    "        if current_tokens + doc_token_estimate > max_context_tokens:\n",
    "            # If it's the first document, include a truncated version\n",
    "            if i == 1:\n",
    "                # Truncate to fit within remaining budget\n",
    "                max_chars = (max_context_tokens - current_tokens) * 4\n",
    "                truncated_text = highlighted_text[:max_chars] + \"...\"\n",
    "                context += f\"Document {i} (Similarity: {similarity:.4f}):\\n{truncated_text}\\n\\n\"\n",
    "            # Otherwise, we've added enough context\n",
    "            break\n",
    "        \n",
    "        # Add the document to context\n",
    "        context += f\"Document {i} (Similarity: {similarity:.4f}):\\n{highlighted_text}\\n\\n\"\n",
    "        current_tokens += doc_token_estimate\n",
    "    \n",
    "    return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answer(question, context, llm_pipeline, max_new_tokens=512):\n",
    "    \"\"\"Generate answer using the LLM with improved prompt engineering\"\"\"\n",
    "    # Analyze question to determine appropriate response structure\n",
    "    question_lower = question.lower()\n",
    "    \n",
    "    # Check for question type\n",
    "    is_factoid = any(w in question_lower for w in ['who', 'what', 'when', 'where', 'which', 'how many', 'how much'])\n",
    "    is_comparison = any(w in question_lower for w in ['compare', 'difference', 'similar', 'versus', 'vs'])\n",
    "    is_how_to = 'how to' in question_lower or 'steps' in question_lower\n",
    "    \n",
    "    # Build a prompt suited to the question type\n",
    "    system_instruction = \"\"\"You are a helpful, accurate assistant. When answering questions:\n",
    "1. Only provide information supported by the context.\n",
    "2. Use simple, clear language to explain complex concepts.\n",
    "3. Be concise but comprehensive.\n",
    "4. If the context doesn't fully answer the question, acknowledge the limitations of the information.\"\"\"\n",
    "\n",
    "    # Add specific instruction based on question type\n",
    "    if is_factoid:\n",
    "        system_instruction += \"\\nFor this factual question, cite the most relevant parts of the context.\"\n",
    "    elif is_comparison:\n",
    "        system_instruction += \"\\nFor this comparison question, clearly outline the similarities and differences.\"\n",
    "    elif is_how_to:\n",
    "        system_instruction += \"\\nFor this procedural question, provide clear step-by-step instructions.\"\n",
    "    \n",
    "    prompt = f\"\"\"{system_instruction}\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "    # Generate answer with the LLM using improved settings\n",
    "    response = llm_pipeline(\n",
    "        prompt,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        do_sample=True,\n",
    "        temperature=0.5,  # Lower temperature for more factual responses\n",
    "        top_p=0.92,\n",
    "        top_k=50\n",
    "    )[0]['generated_text']\n",
    "    \n",
    "    # Extract just the answer part (after the prompt)\n",
    "    answer = response[len(prompt):].strip()\n",
    "    \n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_rag_system(\n",
    "    embeddings_csv_path, \n",
    "    text_column=\"document\", \n",
    "    embedding_column=\"embedding\",\n",
    "    llm_model_id=\"/kaggle/input/llama-3.1/transformers/8b/2\",\n",
    "    top_k=5,\n",
    "    distance_metric='hybrid',\n",
    "    embedding_pooling ='mean_cls',\n",
    "    similarity_threshold=0.55\n",
    "):\n",
    "    \"\"\"Initialize the RAG system components\"\"\"\n",
    "    # Setup environment for memory optimization\n",
    "    os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "    \n",
    "    # Check GPU count\n",
    "    gpu_count = torch.cuda.device_count()\n",
    "    print(f\"Found {gpu_count} GPUs\")\n",
    "    \n",
    "    # Clear GPU caches\n",
    "    for i in range(gpu_count):\n",
    "        with torch.cuda.device(i):\n",
    "            torch.cuda.empty_cache()\n",
    "    \n",
    "    # Load data from CSV\n",
    "    print(f\"Loading embeddings from {embeddings_csv_path}...\")\n",
    "    df = pd.read_csv(embeddings_csv_path)\n",
    "    \n",
    "    # Validate columns\n",
    "    if text_column not in df.columns:\n",
    "        raise ValueError(f\"Text column '{text_column}' not found in CSV\")\n",
    "    \n",
    "    if embedding_column not in df.columns:\n",
    "        raise ValueError(f\"Embedding column '{embedding_column}' not found in CSV\")\n",
    "    \n",
    "    # Parse embeddings\n",
    "    print(\"Parsing embeddings from string format...\")\n",
    "    embeddings = []\n",
    "    for emb_str in df[embedding_column]:\n",
    "        try:\n",
    "            # Try parsing as list literal\n",
    "            emb = np.array(ast.literal_eval(emb_str))\n",
    "        except (ValueError, SyntaxError):\n",
    "            # Fallback to comma-separated parsing\n",
    "            emb = np.array([float(x) for x in emb_str.strip('[]').split(',')])\n",
    "        embeddings.append(emb)\n",
    "    \n",
    "    # Convert to numpy array\n",
    "    embeddings = np.vstack(embeddings)\n",
    "    print(f\"Loaded {len(embeddings)} embeddings with {embeddings.shape[1]} dimensions\")\n",
    "    \n",
    "    # Preprocess embeddings\n",
    "    normalized_embeddings = preprocess_embeddings(embeddings)\n",
    "    \n",
    "    # Load text embeddings model (on GPU 0)\n",
    "    print(\"Loading MatSciBERT for query embeddings...\")\n",
    "    embedding_device = \"cuda:0\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained('m3rg-iitd/matscibert')\n",
    "    embedding_model = AutoModel.from_pretrained('m3rg-iitd/matscibert', trust_remote_code=True).to(embedding_device)\n",
    "    \n",
    "    # Load vocabulary mappings (optional)\n",
    "    vocab_path = '/kaggle/input/vocabmappings/vocab_mappings.txt'\n",
    "    mappings = load_vocab_mappings(vocab_path) if os.path.exists(vocab_path) else {}\n",
    "    \n",
    "    # Create normalizer\n",
    "    normalizer = BertNormalizer(lowercase=False, strip_accents=True, \n",
    "                                clean_text=True, handle_chinese_chars=True)\n",
    "    \n",
    "    # Configure LLM loading parameters\n",
    "    print(f\"Loading LLM from {llm_model_id}...\")\n",
    "    device_map = \"balanced\" if gpu_count >= 2 else \"auto\"\n",
    "    \n",
    "    # Load LLM pipeline with proper configuration\n",
    "    llm_pipeline = transformers.pipeline(\n",
    "        \"text-generation\", \n",
    "        model=llm_model_id, \n",
    "        model_kwargs={\n",
    "            \"torch_dtype\": torch.bfloat16,\n",
    "            \"device_map\": device_map,\n",
    "            \"offload_folder\": \"/tmp/offload\"\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    # Return a configuration dictionary for RAG system\n",
    "    return {\n",
    "        \"df\": df,\n",
    "        \"embeddings\": normalized_embeddings,\n",
    "        \"documents\": df[text_column].tolist(),\n",
    "        \"tokenizer\": tokenizer,\n",
    "        \"embedding_model\": embedding_model,\n",
    "        \"embedding_device\": embedding_device,\n",
    "        \"normalizer\": normalizer,\n",
    "        \"mappings\": mappings,\n",
    "        \"llm_pipeline\": llm_pipeline,\n",
    "        \"config\": {\n",
    "            \"top_k\": top_k,\n",
    "            \"distance_metric\": distance_metric,\n",
    "            \"embedding_pooling\": embedding_pooling,\n",
    "            \"similarity_threshold\": similarity_threshold\n",
    "        }\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_question_answering(rag_system, question):\n",
    "    \"\"\"\n",
    "    Perform RAG-based question answering\n",
    "    \n",
    "    Args:\n",
    "        rag_system: Configuration dictionary from initialize_rag_system\n",
    "        question: User's question\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with answer and top results\n",
    "    \"\"\"\n",
    "    print(\"\\nProcessing query...\")\n",
    "    \n",
    "    # Extract configuration\n",
    "    config = rag_system['config']\n",
    "    \n",
    "    # Get query embedding\n",
    "    query_embedding = get_query_embedding(\n",
    "        question, \n",
    "        rag_system['embedding_model'], \n",
    "        rag_system['tokenizer'], \n",
    "        rag_system['embedding_device'], \n",
    "        pooling=config.get('embedding_pooling', 'mean_cls')\n",
    "    )\n",
    "    \n",
    "    # Find similar texts\n",
    "    similar_texts = find_similar_texts(\n",
    "        query_embedding, \n",
    "        rag_system['embeddings'], \n",
    "        rag_system['documents'], \n",
    "        top_k=config.get('top_k', 5),\n",
    "        distance_metric=config.get('distance_metric', 'hybrid'), \n",
    "        similarity_threshold=config.get('similarity_threshold', 0.6)\n",
    "    )\n",
    "    \n",
    "    # Format context for LLM\n",
    "    context = format_context(similar_texts, question)\n",
    "    \n",
    "    # Generate answer\n",
    "    answer = generate_answer(\n",
    "        question, \n",
    "        context, \n",
    "        rag_system['llm_pipeline']\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        \"answer\": answer,\n",
    "        \"top_results\": similar_texts\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demo_rag_system():\n",
    "    \"\"\"Run a demonstration of the RAG system\"\"\"\n",
    "    # Initialize the RAG system\n",
    "    rag_system = initialize_rag_system(\n",
    "        embeddings_csv_path=\"/kaggle/input/embeddings/chroma_embeddings.csv\",\n",
    "        text_column=\"document\",\n",
    "        embedding_column=\"embedding\"\n",
    "    )\n",
    "    \n",
    "    # Interactive query loop\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"Optimized RAG Question Answering System\")\n",
    "    print(\"=\"*50)\n",
    "    print(\"Type 'exit' to quit\\n\")\n",
    "    \n",
    "    while True:\n",
    "        question = input(\"\\nEnter your question: \")\n",
    "        if question.lower() == 'exit':\n",
    "            break\n",
    "        \n",
    "        print(\"\\nProcessing...\")\n",
    "        try:\n",
    "            result = rag_question_answering(rag_system, question)\n",
    "            \n",
    "            print(\"\\n\" + \"=\"*50)\n",
    "            print(\"ANSWER:\")\n",
    "            print(\"=\"*50)\n",
    "            print(result[\"answer\"])\n",
    "            \n",
    "            print(\"\\n\" + \"=\"*50)\n",
    "            print(\"TOP RELEVANT DOCUMENTS:\")\n",
    "            print(\"=\"*50)\n",
    "            for i, doc in enumerate(result[\"top_results\"], 1):\n",
    "                print(f\"{i}. Similarity: {doc['similarity']:.4f}, Final Score: {doc['final_score']:.4f}\")\n",
    "                # Limit text display to avoid overwhelming console\n",
    "                text_preview = doc[\"text\"]\n",
    "                if len(text_preview) > 300:\n",
    "                    text_preview = text_preview[:300] + \"...\"\n",
    "                print(f\"{text_preview}\\n\")\n",
    "        except RuntimeError as e:\n",
    "            if \"CUDA out of memory\" in str(e):\n",
    "                print(\"\\nERROR: CUDA out of memory. Try clearing GPU cache and reducing model parameters.\")\n",
    "                \n",
    "                # Clear GPU caches\n",
    "                for i in range(torch.cuda.device_count()):\n",
    "                    with torch.cuda.device(i):\n",
    "                        torch.cuda.empty_cache()\n",
    "                        torch.cuda.synchronize()\n",
    "                \n",
    "                print_gpu_utilization()\n",
    "            else:\n",
    "                print(f\"\\nERROR: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    demo_rag_system()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
